{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7599503,"sourceType":"datasetVersion","datasetId":4423767}],"dockerImageVersionId":30683,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer,AutoModelForSequenceClassification,Trainer,TrainingArguments,DataCollatorForSeq2Seq\nimport torch\nfrom torch.utils.data import Dataset\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_path='/kaggle/input/coldata/train.csv'\ndev_path='/kaggle/input/coldata/dev.csv'\nsave_path='/kaggle/working/bert'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = TrainingArguments(\n    output_dir=save_path,\n    per_device_train_batch_size=40,\n    gradient_accumulation_steps=2,\n    logging_steps=10,\n    num_train_epochs=1,\n    save_steps=1000,\n    learning_rate=1e-7,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyData(Dataset):\n    def __init__(self,path):\n        super(MyData,self).__init__()\n        self.input=[]\n        self.output=[]\n        train_data=pd.read_csv(path)\n        for i in range(len(train_data['label'])): \n            self.input.append(tokenizer(train_data['TEXT'][i], padding=\"max_length\", max_length=256))\n            self.output.append(train_data['label'][i])\n    def __len__(self):\n        return len(self.input)\n    def __getitem__(self, index): \n        return dict(\n            input_ids=self.input[index]['input_ids'],\n            attention_mask=self.input[index]['attention_mask'],\n            label=self.output[index],\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset=MyData(train_path)\neval_dataset=MyData(dev_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_dataset[1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=args,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"哈哈哈哈哈天津人听了想打人哈哈哈哈哈哈\"\nencoded_input = tokenizer(text, return_tensors='pt').to('cuda')\noutput = model(**encoded_input)\nprint(output)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T04:20:24.413403Z","iopub.execute_input":"2024-04-16T04:20:24.414038Z","iopub.status.idle":"2024-04-16T04:20:24.438193Z","shell.execute_reply.started":"2024-04-16T04:20:24.414006Z","shell.execute_reply":"2024-04-16T04:20:24.437329Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"SequenceClassifierOutput(loss=None, logits=tensor([[ 1.4390, -2.0650]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n","output_type":"stream"}]}]}